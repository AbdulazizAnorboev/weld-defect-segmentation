{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e79aa14-2791-4360-a9be-6fe73f7a6a40",
   "metadata": {},
   "source": [
    "## Step-by-step Training Notebook for SegFormer on Weld Defect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d050122-1ea9-4074-b5d5-43fb8d48aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "!pip install torch torchvision transformers matplotlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe0a2b-c647-49f6-9964-b14bd4e5f98e",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c935bdc-f09e-47fd-a108-cd619a8da03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, AdamW\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246825c-b10b-43f3-8f02-9d7f615cc802",
   "metadata": {},
   "source": [
    "### Set device (use GPU if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4edbf5-8ee1-4a0e-a18f-77db96a23a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0b534-e63d-4df3-beef-cd4d2b118313",
   "metadata": {},
   "source": [
    "### Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4d533-902e-432c-a6b2-721de1a55a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"  # Pretrained SegFormer model\n",
    "num_classes = 2  # Number of classes for segmentation\n",
    "data_image_size = (512, 512)  # Resize images to 512x512\n",
    "batch_size = 16  # Batch size for training and validation\n",
    "num_epochs = 20  # Total number of training epochs\n",
    "learning_rate = 5e-5  # Learning rate for the optimizer\n",
    "checkpoint_path = \"best_model_checkpoint.pth\"  # Path to save the best model checkpoint\n",
    "\n",
    "# Paths to dataset directories (Update paths to your dataset)\n",
    "train_image_dir = \"path_to_train_images\"\n",
    "train_mask_dir = \"path_to_train_masks\"\n",
    "valid_image_dir = \"path_to_valid_images\"\n",
    "valid_mask_dir = \"path_to_valid_masks\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11860987-506a-44c9-82ee-eb82bfce4af5",
   "metadata": {},
   "source": [
    "### Define a custom dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c31cc3-722e-46e7-86ab-c97d55b934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, feature_extractor):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "        \n",
    "        Args:\n",
    "            image_dir (str): Path to the directory containing images.\n",
    "            mask_dir (str): Path to the directory containing masks.\n",
    "            feature_extractor: Pretrained feature extractor from Hugging Face.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.images = sorted([f for f in os.listdir(image_dir) if f.endswith(\".jpg\")])\n",
    "        self.masks = sorted([f for f in os.listdir(mask_dir) if f.endswith(\".png\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a single sample (image and mask)\"\"\"\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        # Load and process the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        processed_image = self.feature_extractor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "\n",
    "        # Load and process the mask\n",
    "        mask = Image.open(mask_path).resize(data_image_size, Image.NEAREST)\n",
    "        mask_tensor = torch.tensor(np.array(mask), dtype=torch.long)\n",
    "\n",
    "        return {\"pixel_values\": processed_image, \"labels\": mask_tensor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f7e6b-9777-43f6-8366-a3931bce1160",
   "metadata": {},
   "source": [
    "### Initialize feature extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59d0bb-b7dd-44f7-adbb-04f4c53bf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(model_name, reduce_labels=False)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = SegmentationDataset(train_image_dir, train_mask_dir, feature_extractor)\n",
    "valid_dataset = SegmentationDataset(valid_image_dir, valid_mask_dir, feature_extractor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a32691-e332-4564-95a7-0a13ab07a6c1",
   "metadata": {},
   "source": [
    "### Load SegFormer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cec3b-038e-4790-8198-50c91ae172ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_name, ignore_mismatched_sizes=True, num_labels=num_classes\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c204a2d-8589-49ad-a7e6-d50f9ca9e299",
   "metadata": {},
   "source": [
    "### Training and validation loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c28330-64dc-46e2-9ccf-c23be7bb7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_mious = []\n",
    "val_accuracies = []\n",
    "best_miou = 0  # To track the best model\n",
    "\n",
    "# Define metric computation function\n",
    "def compute_metrics(preds, labels, num_classes):\n",
    "    preds = preds.flatten()\n",
    "    labels = labels.flatten()\n",
    "    intersection = torch.zeros(num_classes, dtype=torch.float32)\n",
    "    union = torch.zeros(num_classes, dtype=torch.float32)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        intersection[cls] = ((preds == cls) & (labels == cls)).sum()\n",
    "        union[cls] = ((preds == cls) | (labels == cls)).sum()\n",
    "\n",
    "    miou = (intersection / (union + 1e-6)).mean().item()\n",
    "    pixel_acc = (preds == labels).float().mean().item()\n",
    "    return miou, pixel_acc\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training step\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_miou = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Resize logits to match labels\n",
    "            logits_upsampled = F.interpolate(logits, size=labels.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "            preds = torch.argmax(logits_upsampled, dim=1)\n",
    "\n",
    "            # Compute metrics\n",
    "            miou, accuracy = compute_metrics(preds.cpu(), labels.cpu(), num_classes)\n",
    "            total_miou += miou\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "    avg_miou = total_miou / len(valid_loader)\n",
    "    avg_accuracy = total_accuracy / len(valid_loader)\n",
    "    val_mious.append(avg_miou)\n",
    "    val_accuracies.append(avg_accuracy)\n",
    "\n",
    "    print(f\"Validation mIoU: {avg_miou:.4f}, Pixel Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_miou > best_miou:\n",
    "        best_miou = avg_miou\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Best model saved with mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0afd9-eb65-4866-ad6a-2ada90bed141",
   "metadata": {},
   "source": [
    "### Plot metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8e84f-9fc3-4481-a30d-0a2fe2c958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Epoch\": list(range(1, num_epochs + 1)),\n",
    "    \"Training Loss\": train_losses,\n",
    "    \"Validation mIoU\": val_mious,\n",
    "    \"Validation Accuracy\": val_accuracies\n",
    "})\n",
    "metrics_df.to_csv(\"training_metrics.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(metrics_df[\"Epoch\"], metrics_df[\"Training Loss\"], label=\"Loss\")\n",
    "plt.plot(metrics_df[\"Epoch\"], metrics_df[\"Validation mIoU\"], label=\"mIoU\")\n",
    "plt.plot(metrics_df[\"Epoch\"], metrics_df[\"Validation Accuracy\"], label=\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Training Metrics\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"training_metrics_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acacbd8-c7f7-4811-8fd8-fc01bcb3b97e",
   "metadata": {},
   "source": [
    "### Testing and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c278e4-f814-4652-aba2-cad4af9ca4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, feature_extractor, images, masks, device):\n",
    "    model.eval()\n",
    "    for img_path, mask_path in zip(images, masks):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_mask = Image.open(mask_path)\n",
    "        input_image = feature_extractor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=input_image)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits_upsampled = F.interpolate(\n",
    "            logits,\n",
    "            size=original_mask.size[::-1],  # (width, height) in PIL format\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_mask = torch.argmax(logits_upsampled, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.imshow(image)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Original Mask\")\n",
    "        plt.imshow(np.array(original_mask), cmap=\"gray\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        plt.imshow(predicted_mask, cmap=\"gray\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Test visualization\n",
    "sample_images = [\"path_to_test_image_1\", \"path_to_test_image_2\"]\n",
    "sample_masks = [\"path_to_test_mask_1\", \"path_to_test_mask_2\"]\n",
    "visualize_predictions(model, feature_extractor, sample_images, sample_masks, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_cuda118_torch25",
   "language": "python",
   "name": "py31"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
